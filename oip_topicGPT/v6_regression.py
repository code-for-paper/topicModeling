import pandas as pd
import statsmodels.api as sm
import numpy as np

# è¯»å–ä½ çš„å‡†å¤‡å¥½çš„è¡¨æ ¼
df = pd.read_csv('./data/output/regression_input.csv')

tobinq = 'refer_tobins_q'
tobinq = 'tobinq'

# df['Supplier Collaboration_xrdint'] = df['Supplier Collaboration'] * df['xrdint']
# df['Technology Partnerships_xrdint'] = df['Technology Partnerships'] * df['xrdint']
# OIP ä¸»é¢˜å˜é‡çš„å¹³æ–¹é¡¹ï¼ˆé€‰æ‹©å‡ ä¸ªï¼‰
# df['Supplier Collaboration_sq'] = df['Supplier Collaboration'] ** 2
# df['Technology Partnerships_sq'] = df['Technology Partnerships'] ** 2
# df['IP Licensing_sq'] = df['IP Licensing'] ** 2
# df['Strategic Partnerships_sq'] = df['Strategic Partnerships'] ** 2


# è®¾ç½®è§£é‡Šå˜é‡ï¼ˆåŒ…å«æŽ§åˆ¶å˜é‡å’Œ OIP ä¸»é¢˜ï¼‰

explanatory_fixed_vars = [
    'log_at',
    'xrdint',
    'roa',
    'leverage', 
]

explanatory_topic_vars = [
    # "Supplier & Manufacturing Collaboration",
    # "Customer & Community Engagement",
    # "Strategic & Joint Partnerships",
    # "IP & Technology Licensing",
    # "Industry-Academia & Research Collaboration",
    # "Technology & Platform Ecosystems",
    # "Open Innovation & Ecosystem Strategy",
    "Marketing & Commercialization Cooperation",
    # oip sum
    # 'OIP',
]

other_vars = [
 # R&D interaction
    # "Supplier & Manufacturing Collaboration_xrdint",
    # "Customer & Community Engagement_xrdint",
    # "Strategic & Joint Partnerships_xrdint",
    # "IP & Technology Licensing_xrdint",
    # "Industry-Academia & Research Collaboration_xrdint",
    # "Technology & Platform Ecosystems_xrdint",
    # "Open Innovation & Ecosystem Strategy_xrdint",
    # "Marketing & Commercialization Cooperation_xrdint",
    # 'OIP_xrdint'

    # square
    # "Supplier & Manufacturing Collaboration_sq",
    # "Customer & Community Engagement_sq",
    # "Strategic & Joint Partnerships_sq",
    # "IP & Technology Licensing_sq",
    # "Industry-Academia & Research Collaboration_sq",
    # "Technology & Platform Ecosystems_sq",
    # "Open Innovation & Ecosystem Strategy_sq",
    "Marketing & Commercialization Cooperation_sq",
    # 'OIP_sq'
]



# è®¡ç®—äº¤äº’é¡¹ï¼ˆOIP Ã— xrdintï¼‰
for topic in explanatory_topic_vars:
    interaction_col = f"{topic}_xrdint"
    df[interaction_col] = df[topic] * df['xrdint']

# è®¡ç®—äº¤äº’é¡¹ï¼ˆOIP Ã— roaï¼‰
for topic in explanatory_topic_vars:
    interaction_col = f"{topic}_roa"
    df[interaction_col] = df[topic] * df['roa']

# è®¡ç®—å¹³æ–¹é¡¹ï¼ˆOIP ^ 2ï¼‰
for topic in explanatory_topic_vars:
    quadratic_col = f"{topic}_sq"
    df[quadratic_col] = df[topic] ** 2


explanatory_vars = explanatory_fixed_vars + explanatory_topic_vars + other_vars

# æ•°æ®æ¸…ç†ï¼šå¤„ç†æ— ç©·å¤§å’Œ NaN å€¼
print(f"åŽŸå§‹æ•°æ®è¡Œæ•°: {len(df)}")

# æ£€æŸ¥æ¯åˆ—çš„ç¼ºå¤±å€¼å’Œæ— ç©·å¤§å€¼
for col in [tobinq] + explanatory_vars:
    if col in df.columns:
        inf_count = np.isinf(df[col]).sum()
        nan_count = df[col].isna().sum()
        if inf_count > 0 or nan_count > 0:
            print(f"{col}: {inf_count} ä¸ªæ— ç©·å¤§å€¼, {nan_count} ä¸ª NaN å€¼")

# æ›¿æ¢æ— ç©·å¤§å€¼ä¸º NaN
df = df.replace([np.inf, -np.inf], np.nan)

# åˆ é™¤åŒ…å« NaN å€¼çš„è¡Œ
df_clean = df[[tobinq] + explanatory_vars].dropna()
# ===================== å‰”é™¤å°¾éƒ¨å½±å“ ====================
q_low = df_clean[tobinq].quantile(0.02)
q_high = df_clean[tobinq].quantile(0.99)

# ä¿ç•™æ­£å¸¸èŒƒå›´å†…çš„æ ·æœ¬
df_clean = df_clean[(df_clean[tobinq] >= q_low) & (df_clean[tobinq] <= q_high)]

print(f"æ¸…ç†åŽæ•°æ®è¡Œæ•°: {len(df_clean)}")
print(f"åˆ é™¤äº† {len(df) - len(df_clean)} è¡Œæ•°æ®")

# å‡†å¤‡æ•°æ®
X = df_clean[explanatory_vars]
X = sm.add_constant(X)  # åŠ æˆªè·é¡¹
y = df_clean[tobinq]         # å› å˜é‡

# æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰ inf æˆ– NaN
if np.any(np.isinf(X)) or np.any(np.isnan(X)) or np.any(np.isinf(y)) or np.any(np.isnan(y)):
    print("è­¦å‘Šï¼šæ•°æ®ä¸­ä»ç„¶å­˜åœ¨ inf æˆ– NaN å€¼")
else:
    print("æ•°æ®æ¸…ç†å®Œæˆï¼Œå¯ä»¥è¿›è¡Œå›žå½’åˆ†æž")

# æè¿°æ€§ç»Ÿè®¡åˆ†æž
print("æè¿°æ€§ç»Ÿè®¡åˆ†æžï¼ˆå› å˜é‡å’Œè§£é‡Šå˜é‡ï¼‰ï¼š")
print(df_clean[[tobinq] + explanatory_vars].describe())

# æè¿°æ€§ç»Ÿè®¡åˆ†æžï¼Œä¿å­˜åˆ°csvæ–‡ä»¶
desc_stats = df_clean[[tobinq] + explanatory_vars].describe()
desc_stats.to_csv('./data/output/desc_stats.csv', encoding='utf-8-sig')

print("æè¿°æ€§ç»Ÿè®¡ç»“æžœå·²ä¿å­˜åˆ° './data/output/desc_stats.csv'")

# è®¡ç®—å˜é‡é—´çš„çº¿æ€§ç›¸å…³ç³»æ•°
print("\nå˜é‡é—´çº¿æ€§ç›¸å…³ç³»æ•°çŸ©é˜µï¼š")
corr_matrix = df_clean[[tobinq] + explanatory_vars].corr()
print(corr_matrix)

# æ˜¾è‘—æ€§æ£€éªŒ åŒå°¾ 0.05
from scipy.stats import pearsonr

# ðŸ‘‡ å¼ºåˆ¶æ•´ä¸ªçŸ©é˜µè½¬æ¢ä¸º string ç±»åž‹ï¼ˆé¿å… dtype å†²çªï¼‰
corr_matrix = corr_matrix.astype(str)

# ç”¨å¸¦æ˜¾è‘—æ€§æ ‡è®°çš„å­—ç¬¦ä¸²æ›¿æ¢å†…å®¹ï¼Œåªä¿å­˜ä¸‹ä¸‰è§’
for i in range(len(corr_matrix.index)):
    for j in range(len(corr_matrix.columns)):
        row_name = corr_matrix.index[i]
        col_name = corr_matrix.columns[j]
        
        if i == j:
            # å¯¹è§’çº¿å…ƒç´ 
            corr_matrix.loc[row_name, col_name] = "1.000"
        elif i > j:
            # ä¸‹ä¸‰è§’å…ƒç´ 
            r, p = pearsonr(df_clean[row_name], df_clean[col_name])
            if p < 0.05:
                corr_matrix.loc[row_name, col_name] = f"{r:.3f}*"
            else:
                corr_matrix.loc[row_name, col_name] = f"{r:.3f}"
        else:
            # ä¸Šä¸‰è§’å…ƒç´ è®¾ä¸ºç©ºå­—ç¬¦ä¸²
            corr_matrix.loc[row_name, col_name] = ""

# ä¿å­˜ç›¸å…³ç³»æ•°çŸ©é˜µåˆ°csvæ–‡ä»¶
corr_matrix.to_csv('./data/output/correlation_matrix.csv', encoding='utf-8-sig')
print("ç›¸å…³ç³»æ•°çŸ©é˜µå·²ä¿å­˜åˆ° './data/output/correlation_matrix.csv'")

print('-' * 100)



from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
import pandas as pd

# æå–å¹¶æ ‡å‡†åŒ–è§£é‡Šå˜é‡
X = df_clean[explanatory_vars]
scaler = StandardScaler()
X_scaled_array = scaler.fit_transform(X)

# å°†æ ‡å‡†åŒ–åŽçš„æ•°ç»„è½¬æ¢ä¸º DataFrameï¼Œå¹¶ä¿ç•™åˆ—å
X_scaled_df = pd.DataFrame(X_scaled_array, columns=X.columns, index=X.index)

# æ·»åŠ å¸¸æ•°é¡¹
X_scaled_df = sm.add_constant(X_scaled_df)

# è®¾ç½®å› å˜é‡
y = df_clean[tobinq]  # æˆ–å…¶ä»–ç›®æ ‡å˜é‡

# æ‹ŸåˆOLSæ¨¡åž‹
model = sm.OLS(y, X_scaled_df).fit()

# æ‰“å°ç»“æžœ
print(model.summary())


print('-'*100)
from statsmodels.stats.outliers_influence import variance_inflation_factor

# è®¡ç®—æ¯ä¸ªå˜é‡çš„ VIF
vif_data = pd.DataFrame()
vif_data["variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# è¾“å‡º VIF ç»“æžœ
print("\nVIF åˆ†æžç»“æžœï¼š")
print(vif_data)